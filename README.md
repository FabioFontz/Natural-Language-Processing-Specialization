# Natural Language Processing Specialization (deeplearning.ai)

Welcome! This repository is a **portfolio** of my work from the Coursera/deeplearning.ai *Natural Language Processing Specialization*. Each notebook contains an independent, from‑scratch implementation—in pure Python—of the algorithms explored across the four‑course sequence.

## Why this matters

- **Hands‑on expertise :** I reproduced every graded assignment locally, relying only on open‑source libraries, to internalise the maths and engineering trade‑offs behind modern NLP.
- **Breadth + depth :** The specialization ranges from traditional statistical methods to Transformer‑based architectures, so the code base demonstrates versatility across paradigms.

## Specialization overview

|  №  |  Course                                                               |  Core topics                                                                                                        |  Representative projects                                                                                       |
| --- | --------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |
|  1  |  *Natural Language Processing with Classification and Vector Spaces*  |  Text preprocessing, tokenisation, tf‑idf, Naïve Bayes & logistic regression, word embeddings, semantic similarity  |  • Movie‑review sentiment classifier  • Word‑analogy solver                                                    |
|  2  |  *Natural Language Processing with Probabilistic Models*              |  n‑gram language models, smoothing, Hidden Markov Models, Viterbi & forward‑backward algorithms                     |  • Autocorrect system  • Part‑of‑speech tagger                                                                 |
|  3  |  *Natural Language Processing with Sequence Models*                   |  RNNs, GRUs, LSTMs, Siamese networks, advanced word embeddings (GloVe, word2vec)                                    |  • Recurrent‑network sentiment analyser  • Named‑entity recogniser  • Character‑level language model           |
|  4  |  *Natural Language Processing with Attention Models*                  |  Encoder–decoder architecture, additive & multiplicative attention, self‑attention, Transformers, beam search       |  • Neural machine translation engine  • Abstractive text summariser  • Question‑answering system (BERT‑style)  |

## Directory snapshot

```
C1_Classification_Vector_Spaces/
C2_Probabilistic_Models/
C3_Sequence_Models/
C4_Attention_Models/
```

Each course folder is subdivided by week (e.g. `Week3/sentiment_analysis_rnn.ipynb`) so visitors can quickly locate a notebook in its curricular context.

> *All notebooks run on Python ≥ 3.10 with the usual scientific stack—NumPy, TensorFlow 2, scikit‑learn and NLTK.*

## License

Assignment prompts remain © deeplearning.ai. My code is released under the MIT License for educational and demonstrative purposes.

---

*Questions or feedback? Feel free to open an issue or connect on LinkedIn!*

